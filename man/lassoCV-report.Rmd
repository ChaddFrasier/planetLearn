---
title: "Lasso Regression Model with Cross-Validation"
author: "Chadd Frasier"
date: "5/18/2020"
output: html_document
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# TODO: removed unused libraries. 
# init libraries
library(glmnet)
library(knitr)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyverse)
library(GGally)
library(corrplot)
# document init
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# configuration for all code blocks
knitr::opts_chunk$set(out.width = "100%",
                      text = element_text(size=5),
                      axis.text.x = element_text(angle=90, hjust=1),
                      fig.height = 3,
                      memory.limit(20000000),
                      setwd("D:/cfrasier/work/R"))
```

## Introduction
Machine Learning is the fastest growing field of computer science and its applications can be observed in every science from Marine Biology, where they predict locations of whale gams based on ocean ambient temperatures, to Astrophysics, where we can predict locations of black holes based on tiny variations in image data. Linear models have been used for all kinds of classification and regression problems and the methods are only getting better. First we need to isolate the data rows that have incidence angles of 70$^\circ$ or less. This is because an incidence angles of 90$^\circ$ tells us that the sun is below the horizon and therefore should not be used in any image. We then need to apply the cosine to the Emission, Incidence, Local Emission, and Local Incidence angles of each observation ( row ) in the data. This is done to comply with the driving theory behind these predictions, the Minnaert equation for photometric surface scattering. Minnaert's theory states that the scattering value of the planetary surface is based on the angles that the light particles hit the camera's detectors. This theory is what allows us to convert raw angles values picked up by the camera to a pixel values in a complete image. Dr. Laszlo Kestay predicts that we can generate linear models of the Minnaert equation for the purpose of predicting surface scattering of lunar data set. We need a mission with millions of observations and LROC is a perfect canidate. We use this data set to train differnet linear models on 80% of the observations and testing using the rest of the observations. We perform Cross-Validation to select the most ideal training set for the set and then return the best model. Since Lasso regression uses a lambda and an alpha perameter we will need to perform the CV for each train/test split twince, once for the alpha and another time for the lambda. Then we display the best prediction results. 

## The Data we are using
After processing a large number of IMG files from LROC WAC using ISIS3 on the Nebula Compute Cluster we generate text files containing the phase angle, emission angle, incidence angle, local emission, local incidence, latitude, longitude, Sun azimuthal direction and the spacecraft azimuthal direction. Each file is made up of every IMG file but we use only a single unit and band from each observation because of the large variations in the data between bands. 

```{r, Block 1, echo=TRUE}
# set column names for the data features and label column ( DN )
col_names <- c("DN", "Phase", "Emission", "Incidence", "LEmission", "LIncidence", "Lat", "Long", "SunAz", "CraftAz")
# Read Data in from text file and set column names
data = read.table( "../data/band1unit7.dat", stringsAsFactors=FALSE, 
                      header=FALSE, strip.white=TRUE, sep="", col.names=col_names)
# convert to a matrix
data <- as.data.frame(data)
# filter out incidence angles greater than some number of degrees
data <- data[data$Incidence <= 70, ]
# calculate the cosines of the emission, incidence, local emission, and local incidence
data$Emission = cos(data$Emission)
data$Incidence = cos(data$Incidence)
data$LEmission = cos(data$LEmission)
data$LIncidence = cos(data$LIncidence)
# show the dataset
knitr::kable( data[1:5,], caption = "LROC WAC Pixel Data ( band 1 unit 7 )" )
```

## Lasso Regression vs. Ridge Regression
Ridge regression is an extension of general linear regression. It's basically regular linear regression with L1 Regularization. The important note about Ridge is that it minimizes the error by using a beta value which can be represented as an weight close to 0. The beta determines how that certain variable effects the outcome. In Lasso regression it uses L1-norm which can equal 0 in some cases. That means that some variable weights can also be set to 0, thus eliminating them from the prediction. This has a massive impact on the training potential of any function that has been over trained because the algorithm specifically ignored variables that are considered "noise" and uses the most highly correlated variables for predicting. 

## Lasso Regression w/ K folds
Lasso Regression is an algorithm that focuses on preventing overfitting of data. The secret behind how it does this is the activation function it uses.L1-norm is an absolute value based function which can reach 0, completly ignoring the activation of that feature and removing its influence. 
```{r, LassoReg ,fig.show='hold', echo=TRUE}
# prep data
y.vec <- as.double(data[,1])
X.mat <- as.matrix(data[,-1])
lambda_seq <- 10^seq(3, -3, by = -.125)
colnames(X.mat) <- NULL
# Splitting the data into test and train
set.seed(150)
# train with only a 3/4 of the total data
train = sample( 1:nrow(X.mat), size = (nrow(X.mat) * 0.80) )
test = (-train)
# TODO: create the cross validation for the alpha value
alpha_seq <- seq( 1, 0, by = -0.025 )
alpha_error_vec <- rep(0, times=length(alpha_seq))
r_sq_vec <- rep(0, times=length(alpha_seq))
best_run <- data.frame( y.vec[test], rep(0, times=length( y.vec[test] ) ) )
best_r_sq <- 0
best_alpha <- 0
best_model <- NULL
for ( index in 1:length(alpha_seq) )
{
  # run the 10 fold cross validation of lasso regression to select best lambda
  cv_output <- cv.glmnet(X.mat[train,], y.vec[train], nfolds = 30, 
                         alpha = alpha_seq[index], lambda = lambda_seq)
  
  # identifying best lamda
  best_lam <- cv_output$lambda.min
  
  # Rebuilding the model with best lamda value identified
  lasso_best <- glmnet(X.mat[train,], y.vec[train], alpha = alpha_seq[index], lambda = best_lam)
  
  # calculate predictions
  pred <- predict(lasso_best, s = best_lam, newx = X.mat[test,])
  
  # create the output data matrix and rename
  final <- data.frame( y.vec[test], pred )
  colnames(final) <- c("Actual", "Predicted")
  
  # calculate the R^2 of the function
  RSS <- sum( (final$Predicted - final$Actual) ^ 2 )
  TSS <- sum( (final$Predicted - mean(final$Actual)) ^ 2 ) 
  R_SQ <- 1 - RSS/TSS
  
  if( R_SQ >= max(r_sq_vec) )
  {
    best_run <- final
    best_model <- lasso_best
    best_r_sq <- R_SQ
    best_alpha <- alpha_seq[index]
  }
  
  r_sq_vec <- R_SQ
  
  # calculate error
  lasso_RMSE <- sqrt(sum(final$Predicted - final$Actual)^2) / nrow(final)
  lasso_MSE <- sum(final$Predicted - final$Actual)^2 / nrow(final)
  
  alpha_error_vec[index] <- lasso_MSE
}
# plot the best run we had during the cv for better speed as oposed to printing all runs
actualData = data.frame( c( seq(1, nrow(best_run)) ), best_run$Actual)
predictedData = data.frame( c( seq(1,nrow(best_run)) ) , best_run$Predicted)
cols = c("Predictions", "Point")
colnames(predictedData) = cols
colnames(actualData) = cols
# create and plot guesses vs actual values
p2 <- ggplot() +
  geom_point(data=actualData, aes(x = Predictions ,y = Point), color="red") +
  geom_point(data=predictedData, aes(x = Predictions ,y = Point), color="blue") +
  xlab("Observations") +
  ylab("DN Value") +
  labs(title = "Predicted vs. Actual",
       subtitle = "Blue vs Red")
  ylim(c(min(actualData),max(actualData))) 
plot(p2)
```

## Variables Used for Prediction
These are the biases from the best run
```{r, echo=TRUE}
  # display which variables are being used in the model
  coef( best_model )
# print best r recieved from CV and the error over every alpha
best_r_sq
best_alpha
alpha_error_vec
```

## Conclusion
These results above are the most promising so far. Where the basic single-layer NN overfits the training set a little too much causing about a 5% error on predictions, and the multi-layer NN seems to overtrain so ungodly bad that the error is hardly minimizable to any accurate predictions whatsoever, this Linear Model using Lasso Regression seems to be training very well in coparision to the other models tried so far. 

The reason this is a better model than the NN is because in a neural network, biases between EVERY input node will be manipulated and used in order to create a prediction for the inputs. That means that every value will be taken into consideration for the prediction no matter how low the biaes is. Which intern can cause discrepencies when there are hidden linear relationships between other input variables that the NN does not discover on it's own.

Lasso Regression uses Shrinkage to converge on a central point of the data, usually the mean of the set. This training model encourages simple, sparse models which means that only the variables that are deemed "useful" are actually used in the prediction. We can see this idea in action by viewing this model's biased using the `coef(lasso_best)` command, this command in R prints out the variables' biases to show us which variables in the model are being "ignored". ( The bias is 0 )